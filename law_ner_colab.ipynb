{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "law_ner.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ImLQkMvuqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62d1ab69-8b36-40c0-dcf5-3c871171a09e"
      },
      "source": [
        "%tensorflow_version 1.15\n",
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install keras_bert==0.84.0\n",
        "!pip install keras==2.3.1\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.15`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "Collecting tensorflow-gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow-gpu==1.15.0) (1.15.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.6 (from tensorflow-gpu==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.35.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.33.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (50.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=5b3a9b5d6714a2ce410ff0591c46e93e809b5bf14fab6eb19d29007fce507b66\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorflow-gpu\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 tensorflow-gpu-1.15.0\n",
            "Collecting keras_bert==0.84.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ec/08/bffa03eb899b20bfb60553e4503f8bac00b83d415bc6ead08f6b447e8aaa/keras-bert-0.84.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_bert==0.84.0) (1.18.5)\n",
            "Requirement already satisfied: Keras in /tensorflow-1.15.2/python3.6 (from keras_bert==0.84.0) (2.3.1)\n",
            "Collecting keras-transformer>=0.37.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from Keras->keras_bert==0.84.0) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert==0.84.0) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert==0.84.0) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert==0.84.0) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert==0.84.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert==0.84.0) (1.1.2)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.84.0-cp36-none-any.whl size=36140 sha256=d86fa09ae8084bac8dc47f279314aaea328e34fd02ecbeb9c397a5707cea1d29\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/59/04/12e95257aebfd27f7edaaf65ab7dd57b5f6cadfb183f1a4ccd\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=6b618e3a71d3c2d398c2fd27a691ac3fa20a2571aa15dea348ab548c4629a1d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=d62dc57cf191c9a309801d81f0b5810a5b87bbf16c74ec3e2609c673027808c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=4f879f71fc246e91def129a17e9873520090203cac4929c5e722e65e05306427\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=2e1b1d62e9fffbd3f5e51fb96dfda43759c93a8610c0d8d4c6553fb460f1d3ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=6e17ae3ca2cbe5a789da591c5c3447bfe9fa6f4fea2701cccb985242740c4a10\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=897bf9043c3ccb50ad3d0817c2ad61a70320d6fa7382b3ffaa962b54c7cb635b\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=942453b42f1e8e3675a692ffceba9f52080701e161d3210ad988565845735d10\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.84.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n",
            "Requirement already satisfied: keras==2.3.1 in /tensorflow-1.15.2/python3.6 (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras==2.3.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.5)\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-2nf9fifk\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-2nf9fifk\n",
            "Requirement already satisfied: keras in /tensorflow-1.15.2/python3.6 (from keras-contrib==2.0.8) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101066 sha256=669d71cde8a4820b6ee68854303cc5553ab37e89ef58b075d26f327008d779db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-odjnxyh9/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRpYDGi6viVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a2767c-50aa-40d5-bf78-22b72417fa13"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda, Bidirectional, LSTM, Dense\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_accuracy\n",
        "from keras_bert import Tokenizer\n",
        "from keras_bert import AdamWarmup, calc_train_steps\n",
        "from datetime import datetime\n",
        "import keras\n",
        "from collections import Counter\n",
        "import keras.callbacks\n",
        "import re\n",
        "import codecs\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCu4oZapad_u",
        "outputId": "9155ea6d-c1ff-4ed3-c7aa-d2d1ac0b2cc6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97zEQjk7Ljuj",
        "outputId": "bd00e1f8-0863-40e9-d447-6ef86901cfcd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "0.16218698900001982\n",
            "GPU (s):\n",
            "0.16630181799999377\n",
            "GPU speedup over CPU: 0x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaeQbLCB0DVm",
        "outputId": "39db06a5-0e74-4b5b-f8fc-4214a2c7a48c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/chinese_L-12_H-768_A-12/\n",
        "%ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/chinese_L-12_H-768_A-12\n",
            "bert_config.json                     bert_model.ckpt.index  vocab.txt\n",
            "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2pj0XJQviVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40391f38-5dfa-4ef9-9c54-91193055c529"
      },
      "source": [
        "# load and clean data\n",
        "data = pd.read_excel(r'/gdrive/My Drive/law/判決書.xlsx')\n",
        "data['content'] = data['content'].str.replace('[ 　]', '')\n",
        "data['content'] = data['content'].str.replace(r'\\xa0', '')\n",
        "data['content'] = data['content'].str.replace(r'\\n\\n', '\\n')\n",
        "data['content'] = data['content'].str.replace(r'\\n[0-9]{1,}\\r', '')\n",
        "data['content'] = data['content'].str.replace(r'書記官.*', '')\n",
        "data['content'] = data['content'].str.replace(r'如不服本判決.*', '')\n",
        "data['content'] = data['content'].str.replace(r'[┌├└┐┤┘┬┴│┼─]', '')\n",
        "# 因為之前訓練model會訓練出位置的訊息，因此決定不取固定位置的文本\n",
        "# 讓文本取的長度都是510，但位置不同\n",
        "def content_slice(contents):\n",
        "    contents_slice = []\n",
        "    for content in contents:\n",
        "        a = random.randint(50, len(content))\n",
        "        contents_slice.append(content[a:a+510])\n",
        "\n",
        "    return contents_slice\n",
        "data['content'] = content_slice(data['content'])\n",
        "\n",
        "test = data.iloc[25000:]\n",
        "data = data.iloc[:25000, ]\n",
        "# data.iloc[0]\n",
        "# data['content'].iloc[0]\n",
        "# data['content'].iloc[-1]\n",
        "# data['url'].iloc[0]\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAZajbdeviWK",
        "scrolled": true
      },
      "source": [
        "# given BERT data path\n",
        "bert_dir = r'/gdrive/My Drive/chinese_L-12_H-768_A-12/'\n",
        "config_path = os.path.join(bert_dir, 'bert_config.json')\n",
        "checkpoint_path = os.path.join(bert_dir, 'bert_model.ckpt')\n",
        "dict_path = os.path.join(bert_dir, 'vocab.txt')\n",
        "\n",
        "# given globel para\n",
        "maxlen = 512\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "input_shape = (maxlen, )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUARqtIHviV1",
        "scrolled": false
      },
      "source": [
        "def create_tokenizer(dict_path):\n",
        "    '建BERT的字典'\n",
        "    token_dict = {}\n",
        "    with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
        "        for line in reader:\n",
        "            token = line.strip()\n",
        "            token_dict[token] = len(token_dict)\n",
        "            \n",
        "    return token_dict\n",
        "\n",
        "# create BERT tokenizer\n",
        "token_dict = create_tokenizer(dict_path)\n",
        "tokenizer = Tokenizer(token_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "736t1ZQNviV6"
      },
      "source": [
        "def encoded(tokenizer, data, maxlen):\n",
        "    '把新聞轉成 model input, 三個向量。只取後512個字'\n",
        "    tokens, segments, masks = [], [], []\n",
        "    for content in data['content']:\n",
        "        # break\n",
        "        if len(content) > 510:\n",
        "            content = content[-510:]\n",
        "        token, segment = tokenizer.encode(\n",
        "            first=content, max_len=maxlen\n",
        "            )\n",
        "        mask = [1] * len(token)\n",
        "        tokens.append(token)\n",
        "        segments.append(segment)\n",
        "        masks.append(mask)\n",
        "            \n",
        "    return tokens, segments, masks\n",
        "\n",
        "# 產生input\n",
        "token_input, segment_input, mask_input = encoded(tokenizer, data, maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHfqIOlUJC7R"
      },
      "source": [
        "def create_label_crf(tokenizer, token_input, judge, maxlen):\n",
        "    '把法官名稱轉成model label，兩個向量，分別記錄名字的開始、結束位置'\n",
        "    # 產生空list\n",
        "    # [[0] * maxlen]*len(token_input) 類似這種寫法，是淺複製\n",
        "    # 會成為len(token_input)個一樣的list\n",
        "    # start_label = list(temp) * len(token_input)也是淺複製\n",
        "    temp = [0] * maxlen\n",
        "    label = []\n",
        "    for i in range(len(token_input)):\n",
        "        label.append(list(temp))\n",
        "    \n",
        "    # 利用已知的法官名稱，標記出兩個one-hot的vector\n",
        "    # start, end分別記錄一個法官名字的開始與結尾\n",
        "    for a, (tokens, ju) in enumerate(zip(token_input, judge)): # 每個文本\n",
        "        for name in ju: # 每個人名\n",
        "            name_tokens = tokenizer.encode(name)[0][1:-1] # 名字轉換成tokenid\n",
        "            # 對整個list遍歷，找到len(name_tokens)裡面的組合\n",
        "            for tt, token in enumerate(tokens):\n",
        "                # 找到開頭一樣\n",
        "                if token == name_tokens[0]:\n",
        "                    # 且後面幾個也都一樣\n",
        "                    if tokens[tt:(tt+len(name_tokens))] == name_tokens:\n",
        "                        # 紀錄下來\n",
        "                        for j in range(tt, (tt+len(name_tokens))):\n",
        "                            label[a][j] = 2\n",
        "                        label[a][tt] = 1\n",
        "                        \n",
        "    return label\n",
        "\n",
        "# 提取法官當作label\n",
        "judge = data['content'].str[-510:].str.findall(r'\\n法官(.{2,5})\\r')\n",
        "# 產生label\n",
        "label = create_label_crf(tokenizer, token_input, judge, maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNlUhLvU4ZVm",
        "outputId": "078d578c-e925-4269-e395-3292b5abc7af"
      },
      "source": [
        "# 人名次數分佈\r\n",
        "judge.map(len).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    20290\n",
              "1     2704\n",
              "2     1588\n",
              "4      403\n",
              "3       15\n",
              "Name: content, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2210yaYZviXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c05ce59-f010-481c-d69b-801004a63b9e"
      },
      "source": [
        "a = 0\n",
        "idx = label[a].index(1)\n",
        "st, ed = idx-10, idx+40\n",
        "print(token_input[a][st:ed])\n",
        "print(segment_input[a][st:ed])\n",
        "print(mask_input[a][st:ed])\n",
        "print(label[a][st:ed])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2431, 2182, 1161, 7269, 3791, 2135, 1425, 4247, 3791, 2135, 2528, 3208, 7093, 3791, 2135, 6972, 989, 1923, 3791, 2135, 3360, 2617, 2255, 3791, 2135, 6258, 7093, 1313, 3315, 816, 3633, 3315, 6349, 3209, 5645, 1333, 3315, 4192, 4530, 704, 5836, 3696, 1751, 8965, 2399, 8108, 3299, 8143, 3189, 102]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 2, 2, 0, 0, 1, 2, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tertj5zIqr1",
        "outputId": "5923a2f3-046a-4c07-aae0-84c61b37f36c"
      },
      "source": [
        "# tokenizer.encode('例稿名稱：臺灣屏東地方法院公示', max_len=512)\n",
        "print(type(token_input), type(segment_input), type(mask_input), type(label))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> <class 'list'> <class 'list'> <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoaf_9ZCviXU"
      },
      "source": [
        "def bert_BiLSTM_CRF_model():\n",
        "    'NER模型'\n",
        "    ner_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen)\n",
        "    bert_output = ner_model.layers[-9].output\n",
        "    X = Lambda(lambda x: x[:, 0: input_shape[0]])(bert_output)\n",
        "    X = Bidirectional(LSTM(128, return_sequences=True))(X)\n",
        "\n",
        "    output = CRF(3, sparse_target = True)(X)\n",
        "    ner_model = Model(ner_model.input, output)\n",
        "    \n",
        "    for layer in ner_model.layers:\n",
        "        layer.trainable = False\n",
        "    ner_model.layers[-1].trainable = True\n",
        "    ner_model.layers[-2].trainable = True\n",
        "    \n",
        "    return ner_model\n",
        "\n",
        "model = bert_BiLSTM_CRF_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7OE9od8J1Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395ab9eb-42be-4b76-f2c1-943d86cdd4be"
      },
      "source": [
        "total_steps, warmup_steps = calc_train_steps(\n",
        "    num_example=data.shape[0],\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    warmup_proportion=0.1,\n",
        ")\n",
        "\n",
        "optimizer = AdamWarmup(total_steps, warmup_steps, lr=1e-3, min_lr=1e-5)\n",
        "\n",
        "callback_list = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "          #,ModelCheckpoint(filepath='AML_bert.h5', monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZp6POGAJ7uM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34303420-504b-4dbe-e008-b5b78596a052"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      (None, 512)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 512, 768), ( 16226304    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 512, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 512, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 512, 768)     393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 512, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 512, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 512, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 512, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 512, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 512, 768)     0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 512, 256)     918528      lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "crf_1 (CRF)                     (None, 512, 3)       786         bidirectional_1[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 102,596,370\n",
            "Trainable params: 919,314\n",
            "Non-trainable params: 101,677,056\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAs1OzD3iDaM"
      },
      "source": [
        "label1 = keras.utils.to_categorical(label, num_classes=3, dtype='float32')\n",
        "label1[a][st:ed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJxsC-Q0J72W",
        "outputId": "f04da3c6-ff57-4772-ae04-70f2d69bee6e"
      },
      "source": [
        "model.fit([token_input, segment_input, mask_input], label1,\n",
        "       epochs=epochs, batch_size=batch_size,\n",
        "          #validation_split=0.1\n",
        "          callbacks=callback_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 1502s 60ms/step - loss: -0.0015 - crf_accuracy: 0.9999\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 1522s 61ms/step - loss: -0.0019 - crf_accuracy: 0.9999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f64d12c8ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "merrPOA-vHI8"
      },
      "source": [
        "model.save(r'/gdrive/My Drive/law/law_ner.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzK4-t6tV88q"
      },
      "source": [
        "model.load_weights(r'/gdrive/My Drive/law/law_ner.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22irQMd20xdV"
      },
      "source": [
        "# 檢視train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qshLMIDgUk3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc416d0-89fb-4ff2-a445-54a7f43afd99"
      },
      "source": [
        "prediction = model.predict([token_input, segment_input, mask_input])\n",
        "prediction[a][st:ed]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnvLQ8vmffKw"
      },
      "source": [
        "import numpy as np\n",
        "y_pred = np.argmax(prediction, axis=-1)\n",
        "# y_pred = y_pred-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gUVvKMWfkrr",
        "outputId": "287c8d2f-7458-4f9b-d9d1-c480378f78ef"
      },
      "source": [
        "print(len(label), len(label[0]))\n",
        "print(y_pred.shape)\n",
        "print(len(token_input), len(token_input[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 512\n",
            "(25000, 512)\n",
            "25000 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS6tTq3uK1rw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7a36ec-fb34-4110-dc78-5a9824a5e2ae"
      },
      "source": [
        "print(label1[a][st:(ed-20)])\n",
        "y_pred[a][st:ed]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHYY1Y6yCXWf",
        "outputId": "28359495-d446-4ebf-86d8-d8f2a69b9a43"
      },
      "source": [
        "cates = y_pred[0]\n",
        "tokens = token_input[0]\n",
        "np.where(cates!=0)[0]\n",
        "# token_input[0][404:407]\n",
        "# token_dict_inv[token_input[0][404:407][0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([198, 199, 200, 203, 204, 205, 208, 209, 210, 213, 214, 215])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-TkrFUYCzdE"
      },
      "source": [
        "# token_id map回 token\n",
        "token_dict_inv = {value: key for key, value in token_dict.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSfEf_R_YJmY"
      },
      "source": [
        "def get_name(token_input, y_pred):\n",
        "    '拼湊pred的名稱'\n",
        "    name_list = []\n",
        "    # 每篇文\n",
        "    for tokens, cates in zip(token_input, y_pred):\n",
        "        # 找出分類不為0的\n",
        "        temp_name_list= []\n",
        "        name_index = np.where(cates!=0)[0]\n",
        "        name = '' # 紀錄名字\n",
        "        a = 0 # 紀錄是否連續\n",
        "        for idx in name_index: # 不為0的位置\n",
        "            # print(idx)\n",
        "            if a==0 or idx==a+1: # a=0代表開頭, a==a+1代表連續位置\n",
        "                name += token_dict_inv[tokens[idx]].replace('#', '') # 找出是什麼字\n",
        "                a = idx\n",
        "                if idx == name_index[-1]: # 最後位置\n",
        "                    temp_name_list.append(name)\n",
        "            else:\n",
        "                temp_name_list.append(name)\n",
        "                a = 0\n",
        "                name = ''\n",
        "                name += token_dict_inv[tokens[idx]].replace('#', '') # 找出是什麼字\n",
        "        name_list.append(temp_name_list)\n",
        "        \n",
        "    return name_list\n",
        "name_list = get_name(token_input, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjHX1_YxXpMz",
        "outputId": "6fce315f-1898-44c5-d1b3-d027511a3914"
      },
      "source": [
        "data1 = data.copy()\n",
        "data1['judge'] = judge\n",
        "data1['pred'] = name_list\n",
        "print(data1.iloc[0])\n",
        "\n",
        "# 看看不一樣的地方\n",
        "data2 = data1[data1['judge'] != data1['pred']]\n",
        "print(data2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title                      司法院--刑事補償 109 年度 台覆 字第 101 號刑事決定書\n",
            "url        data.aspx?ro=0&q=7a56f5a4cdd68d79ef3ef69427141...\n",
            "content    審人即補償請求人劉傳文（下稱聲請人）\\r\\n因違反毒品危害防制條例案件，請求刑事補償，原決定...\n",
            "judge                                   [徐昌錦, 鄭傑夫, 林恩山, 許錦印]\n",
            "pred                                    [徐昌錦, 鄭傑夫, 林恩山, 許錦印]\n",
            "Name: 0, dtype: object\n",
            "(327, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dvjwk9in97q",
        "outputId": "e181e43a-071f-47ae-99c7-b44092550184"
      },
      "source": [
        "# patt > model\n",
        "patt_longer_model = data2.loc[data2['judge'].map(len) > data2['pred'].map(len)]\n",
        "# patt < model\n",
        "patt_shorter_model = data2.loc[data2['judge'].map(len) < data2['pred'].map(len)]\n",
        "# patt == model，但不相同\n",
        "patt_notequal_model = data2.loc[data2['judge'].map(len) == data2['pred'].map(len)]\n",
        "\n",
        "print(patt_longer_model.shape, patt_shorter_model.shape, patt_notequal_model.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 5) (131, 5) (176, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "-euAj_Pipfbn",
        "outputId": "eba1bc15-1d72-4048-9777-f51e3d84d511"
      },
      "source": [
        "# patt > model，模型未抓到\n",
        "a = 3\n",
        "print(patt_longer_model.iloc[a, [3, 4]])\n",
        "patt_longer_model.iloc[a, 2][-510:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "judge    [陳文爵]\n",
            "pred        []\n",
            "Name: 644, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'序後，得命司法事務官進行清算程序，消費\\r\\n者債務清理條例第16條第1項前段定有明定，爰併裁定如主\\r\\n文第3項。\\r\\n中華民國109年11月17日\\r\\n臺灣臺中地方法院民事庭\\r\\n法官陳文爵\\r\\n上為正本係照原本作成\\r\\n如不服本裁定關於生活限制部分，應於送達後10日內以書狀向本\\r\\n院提出抗告，並繳納抗告費新臺幣1千元；關於開始清算及命司\\r\\n法事務官進行清算程序部分，不得抗告。\\r\\n本裁定已於109年11月17日下午2時公告。\\r\\n中華民國109年11月17日\\r\\n\\n附件：清算債務人之生活限制\\r\\n\\r\\n准許清算之債務人，在本件清算程序終止或終結前，應受下列之生活限制：\\r\\n\\r\\n一、不得為奢靡浪費之消費活動。\\r\\n\\r\\n二、不得為賭博或為其他投機行為。\\r\\n\\r\\n三、不得為不動產之處分。\\r\\n\\r\\n四、不得為金錢借貸之行為。\\r\\n\\r\\n五、不得搭乘計程車、高鐵及航空器，但債務人能提出確實證據證明因工作所需\\r\\n且未因此減少債權人受償金額者，不在此限。\\r\\n\\r\\n六、不得從事國外遊學或出國旅遊等消費行為。\\r\\n\\r\\n七、不得投資金融商品（例如股票、基金等）。\\r\\n\\r\\n八、不得從事逾越通常生活程度之贈與。\\r\\n\\r\\n九、其他經本院限制之行為。\\r\\n\\r\\n\\n\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "xrLCIRoELj1P",
        "outputId": "f8460019-1bd0-4d82-c8fc-c76d949e9669"
      },
      "source": [
        "# patt == model，長度一致卻不相同\n",
        "a = 3\n",
        "print(patt_notequal_model.iloc[a, [3, 4]])\n",
        "patt_notequal_model.iloc[a, 2][-510:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "judge        [吳佳齡, 周霙蘭]\n",
            "pred     [吳佳齡, 周[UNK]蘭]\n",
            "Name: 501, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'371號判例首揭判例意旨，自不發生\\r\\n回復原狀之問題，從而，聲請人聲請回復原狀亦屬無理由，\\r\\n應予駁回。\\r\\n六、另上開判決雖未合法送達，惟聲請人業於106年1月24日至七\\r\\n堵派出所領取判決，並於106年1月25日具狀提起上訴，有上\\r\\n訴狀首頁所載之本院收文戳章在卷可憑，已生合法上訴之效\\r\\n力，且有利於聲請人，本院茲因郵務機關漏未向本院陳明上\\r\\n情，致本院收受聲請人前開上訴案件時無從發見誤為逾期，\\r\\n而於106年2月10日從程序上為駁回上訴之裁定（見本院106\\r\\n年度聲字第25號卷第26-1頁），此種程序上裁定，不發生實\\r\\n質上確定力，聲請人之上訴不因而失效，自毋庸先依非常上\\r\\n訴程序撤銷（最高法院25年上字第3231號判例、同院80年11\\r\\n月5日第5次刑事庭會議決議參照），併予敘明。\\r\\n據上論斷，依刑事訴訟法第69條第1項前段，裁定如主文。\\r\\n中華民國10年6月8日\\r\\n刑事第五庭審判長法官齊潔\\r\\n法官吳佳齡\\r\\n法官周霙蘭\\r\\n以上正本證明與原本無異。\\r\\n對於本件裁定如有不服，應於收受送達後5日內，向本院提出抗\\r\\n告書狀，敘述抗告之理由，抗告於臺灣高等法院。\\r\\n中華民國106年6月8日\\r\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "nTMZq3t5MDo-",
        "outputId": "acb3eea7-8fcf-436a-8091-8a247cba3867"
      },
      "source": [
        "# patt < model，規則未抓到\n",
        "a = 3\n",
        "print(patt_shorter_model.iloc[a, [3, 4]])\n",
        "patt_shorter_model.iloc[a, 2][-510:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "judge       []\n",
            "pred     [林柏泓]\n",
            "Name: 81, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'為最輕本刑5年以上有期徒刑之罪，基於一般人畏懼長期刑執行之心理，尚難排除為免執行而逃亡之可能，是前項羈押原因依然存在。再審酌羈押之目的在保全刑事審判及執行之進行，並確保刑事審判機關得以依法從事犯罪事實之調查與認定，及擔保執行之必要性，本案現尚未確定，仍有保全審判進行及擔保未來確定後執行之必要性。至於被告雖辯稱其罹患痛風，長期服藥，且名下財產均遭扣押，並無逃亡之虞云云。然查，被告之疾病僅屬慢性病症，不易造成生命危害，亦得服藥控制疼痛，且被告本案遭扣押之物均係名義上屬其所有之財產，但仍不能排除接受親友援助或以其他方式取得接濟之可能，是上開理由，均無解於延長其羈押期間之必要性。經權衡國家刑事司法權之有效行使、被告人身自由之私益及防禦權受限制之程度後，認依目前訴訟進行程度，尚不能以具保等處分代替羈押，而有繼續羈押之必要。三、綜上，本件羈押原因尚未消滅，尚有羈押被告之原因及必要，爰裁定如主文。據上論斷，依刑事訴訟法第108條第1項、第5項，裁定如主文。中華民國109年11月16日刑事第五庭審判長法官王屏夏法官戴嘉清法官林柏泓以上正本證明與原本無異。如不服本裁定，應於收受送達後五日內向本院提出抗告狀。\\n\\n\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q20xtAGr0e_q"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8wm1eac0qik"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# token_id map回 token\r\n",
        "token_dict_inv = {value: key for key, value in token_dict.items()}\r\n",
        "\r\n",
        "def get_name(token_input, y_pred):\r\n",
        "    '拼湊pred的名稱'\r\n",
        "    name_list = []\r\n",
        "    # 每篇文\r\n",
        "    for tokens, cates in zip(token_input, y_pred):\r\n",
        "        # 找出分類不為0的\r\n",
        "        temp_name_list= []\r\n",
        "        name_index = np.where(cates!=0)[0]\r\n",
        "        name = '' # 紀錄名字\r\n",
        "        a = 0 # 紀錄是否連續\r\n",
        "        for idx in name_index: # 不為0的位置\r\n",
        "            # print(idx)\r\n",
        "            if a==0 or idx==a+1: # a=0代表開頭, a==a+1代表連續位置\r\n",
        "                name += token_dict_inv[tokens[idx]].replace('#', '') # 找出是什麼字\r\n",
        "                a = idx\r\n",
        "                if idx == name_index[-1]: # 最後位置\r\n",
        "                    temp_name_list.append(name)\r\n",
        "            else:\r\n",
        "                temp_name_list.append(name)\r\n",
        "                a = 0\r\n",
        "                name = ''\r\n",
        "                name += token_dict_inv[tokens[idx]].replace('#', '') # 找出是什麼字\r\n",
        "        name_list.append(temp_name_list)\r\n",
        "        \r\n",
        "    return name_list\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlipBdMdw7Kl",
        "outputId": "df6f215b-cd4a-4c18-954b-7f3a42ce3d80"
      },
      "source": [
        "token_test, segment_test, mask_test = encoded(tokenizer, test, maxlen)\n",
        "\n",
        "t = time.time()\n",
        "prediction_test = model.predict([token_test, segment_test, mask_test])\n",
        "time.time() - t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.10316681861877"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Zg3zMYXiPP"
      },
      "source": [
        "judge_test = test['content'].str[-510:].str.findall(r'\\n法官(.{2,5})\\r')\r\n",
        "label_test = create_label_crf(tokenizer, token_test, judge_test, maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzfvGVMZX65_",
        "outputId": "8ba64813-e2a2-4483-c7c0-a64ddd8a7900"
      },
      "source": [
        "b = 6\r\n",
        "idx_test = label_test[b].index(1)\r\n",
        "st_test = idx_test-10\r\n",
        "ed_test = idx_test+10\r\n",
        "token_test[b][st_test:ed_test]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2832,\n",
              " 1765,\n",
              " 3175,\n",
              " 3791,\n",
              " 7368,\n",
              " 3696,\n",
              " 752,\n",
              " 2431,\n",
              " 3791,\n",
              " 2135,\n",
              " 7376,\n",
              " 2134,\n",
              " 6545,\n",
              " 1381,\n",
              " 4158,\n",
              " 3633,\n",
              " 3315,\n",
              " 913,\n",
              " 4212,\n",
              " 1333]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyAsoajGY7ms",
        "outputId": "a0778ba7-5ded-4c3e-b46b-63e190c84ae0"
      },
      "source": [
        "print(test.iloc[b, 2][st_test:ed_test])\r\n",
        "print([token_dict_inv[tt] for tt in token_test[b][st_test:ed_test]])\r\n",
        "print(token_test[b][st_test:ed_test])\r\n",
        "print(label_test[b][st_test:ed_test])\r\n",
        "print(y_pred_test[b][st_test:ed_test])\r\n",
        "name_list_test[b]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "十九年二月十五日\r\n",
            "臺灣南投地方法院民事\n",
            "['投', '地', '方', '法', '院', '民', '事', '庭', '法', '官', '陳', '宗', '賢', '右', '為', '正', '本', '係', '照', '原']\n",
            "[2832, 1765, 3175, 3791, 7368, 3696, 752, 2431, 3791, 2135, 7376, 2134, 6545, 1381, 4158, 3633, 3315, 913, 4212, 1333]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0]\n",
            "[ 0  0  0  0  0  0  0  0  0  0 -1 -1 -1  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['條第2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR4yU7UgrRnR",
        "outputId": "9154de4e-ad5c-4e86-8b71-8a1d3b793f35"
      },
      "source": [
        "y_pred_test = np.argmax(prediction_test, axis=-1)\n",
        "y_pred_test = y_pred_test-1\n",
        "name_list_test = get_name(token_test, y_pred_test)\n",
        "\n",
        "\n",
        "test1 = test.copy()\n",
        "test1['judge'] = judge_test\n",
        "test1['pred'] = name_list_test\n",
        "print(test1.iloc[0])\n",
        "print(test1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title                           臺灣南投地方法院 89 年度 訴 字第 94 號民事判決\n",
            "url        data.aspx?ro=440&q=7a56f5a4cdd68d79ef3ef694271...\n",
            "content    確認印鑑證明無效等\\r\\n\\n\\n\\n\\n臺灣南投地方法院民事判決八十九年度訴字第九四號\\r...\n",
            "judge                                                     []\n",
            "pred                                                      []\n",
            "Name: 25000, dtype: object\n",
            "(1471, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw0x436Xxl9P",
        "outputId": "0499dc03-feb0-47b0-d003-5e09e5253162"
      },
      "source": [
        "# 看看不一樣的地方\n",
        "test2 = test1.loc[test1['judge'] != test1['pred']]\n",
        "\n",
        "# 一樣都有 = True Positive\n",
        "print(sum((test1['judge'] == test1['pred']) & (test1['judge'].map(len) > 0)))\n",
        "\n",
        "# 一樣都沒有 = True Nagative\n",
        "print(sum((test1['judge'] == test1['pred']) & (test1['judge'].map(len) == 0)))\n",
        "\n",
        "# 不一樣，真實多於預測 = False Nagative\n",
        "print(sum(test1['judge'].map(len) > test1['pred'].map(len)))\n",
        "\n",
        "# 不一樣，預測多於真實 = False Positive\n",
        "print(sum(test1['judge'].map(len) < test1['pred'].map(len)))\n",
        "\n",
        "# 數量一樣，但內容不完全一樣\n",
        "test2.loc[test2['judge'].map(len) == test2['pred'].map(len)].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "261\n",
            "1192\n",
            "5\n",
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDoKXsiucbj3",
        "outputId": "ebdf5b1b-4a87-4c4f-ec7b-40af02108b3e"
      },
      "source": [
        "# precision\r\n",
        "print(f'precision: {261/(261+8)}')\r\n",
        "# recall\r\n",
        "print(f'recall: {261/(261+5)}')\r\n",
        "# f1_score\r\n",
        "print(f'f1_score: {2*(0.97*0.98)/(0.97+0.98)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision: 0.9702602230483272\n",
            "recall: 0.981203007518797\n",
            "f1_score: 0.974974358974359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StK-uvFsbLxW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ExEHvQBUgzR",
        "outputId": "80c4b14a-cc6f-4aeb-f895-eae46d8800a6"
      },
      "source": [
        "# patt > model\r\n",
        "patt_longer_model = test2.loc[test2['judge'].map(len) > test2['pred'].map(len)]\r\n",
        "# patt < model\r\n",
        "patt_shorter_model = test2.loc[test2['judge'].map(len) < test2['pred'].map(len)]\r\n",
        "# patt == model，但不相同\r\n",
        "patt_notequal_model = test2.loc[test2['judge'].map(len) == test2['pred'].map(len)]\r\n",
        "\r\n",
        "print(patt_longer_model.shape, patt_shorter_model.shape, patt_notequal_model.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 5) (8, 5) (5, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "pw2_G70YVMSl",
        "outputId": "cdb86d66-eea1-47ef-ce36-5899522dd17c"
      },
      "source": [
        "patt_longer_model.iloc[:, 2:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>judge</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25302</th>\n",
              "      <td>0元」，見警\\r\\n卷第23頁），但告訴人表示尚未修繕，復因另行租車使用，\\r\\n每個月要4...</td>\n",
              "      <td>[粘凱庭]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25570</th>\n",
              "      <td>本院107年度上\\r\\n字第197號）之上訴人，為依法得聲請閱覽卷宗之人，復據\\r\\n聲請人...</td>\n",
              "      <td>[黃義成, 藍雅清]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25584</th>\n",
              "      <td>0,500元，有稅務電子閘門財產調件明\\r\\n細表在卷（見原審補字卷第63頁）可查，顯高於債...</td>\n",
              "      <td>[陳春長, 林富郎]</td>\n",
              "      <td>[陳春長]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25585</th>\n",
              "      <td>應徵裁判費。查本件訴訟標的金額為新臺幣\\r\\n（下同）141,887元，應徵裁判費2,325...</td>\n",
              "      <td>[王浦傑, 張季芬]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25709</th>\n",
              "      <td>\\r\\n法官劉台安\\r\\n法官賴劍毅\\r\\n右為正本係照原本作成\\r\\n不得上訴\\r\\n中華...</td>\n",
              "      <td>[劉台安, 賴劍毅]</td>\n",
              "      <td>[賴劍毅]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content       judge   pred\n",
              "25302  0元」，見警\\r\\n卷第23頁），但告訴人表示尚未修繕，復因另行租車使用，\\r\\n每個月要4...       [粘凱庭]     []\n",
              "25570  本院107年度上\\r\\n字第197號）之上訴人，為依法得聲請閱覽卷宗之人，復據\\r\\n聲請人...  [黃義成, 藍雅清]     []\n",
              "25584  0,500元，有稅務電子閘門財產調件明\\r\\n細表在卷（見原審補字卷第63頁）可查，顯高於債...  [陳春長, 林富郎]  [陳春長]\n",
              "25585  應徵裁判費。查本件訴訟標的金額為新臺幣\\r\\n（下同）141,887元，應徵裁判費2,325...  [王浦傑, 張季芬]     []\n",
              "25709  \\r\\n法官劉台安\\r\\n法官賴劍毅\\r\\n右為正本係照原本作成\\r\\n不得上訴\\r\\n中華...  [劉台安, 賴劍毅]  [賴劍毅]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "JZLEmIuwVWX_",
        "outputId": "bdfe1752-eb7c-49db-85c1-7f97a446d2ea"
      },
      "source": [
        "patt_shorter_model.iloc[:, 2:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>judge</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25165</th>\n",
              "      <td>法官陳嘉惠\\r\\n右正本證明與原本無異\\r\\n中華民國88年5月31日\\r\\n法院\\n上抄本...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[陳嘉惠]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25461</th>\n",
              "      <td>情形，應於提起上訴或委任時釋明之。上訴人未依第1項、第2項規定委任訴訟代理人，或雖依第2項委...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[羅立德]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25462</th>\n",
              "      <td>6,002元。茲命上訴人於收受本裁定正本送達後5日內補正委任律師或具律師資格關係人之委任狀及...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[羅立德]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25463</th>\n",
              "      <td>萬5,655元。茲命上訴人於收受本裁定正本之日起5日內，如數向本院補繳，並提出委任律師或具律...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[何君豪]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25466</th>\n",
              "      <td>109年12月24日上午9時30分在本院第12法庭行準備程序，特此裁定。中華民國109年11...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[許勻睿]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25473</th>\n",
              "      <td>律師為訴訟代理人。但上訴人或其法定代理人具有律師資格者，不在此限。上訴人之配偶、三親等內之血...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[朱美[UNK], 何君豪]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25719</th>\n",
              "      <td>庭法官黃書苑\\r\\n右正本證明與原本無異\\r\\n本裁定不得抗告\\r\\n中華民國八十九年一月五...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[黃書苑]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25876</th>\n",
              "      <td>B法官黃麟倫\\r\\n右正本證明與原本無異。\\r\\n對於本件判決如有不服，應於收受送達後二十日...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[黃麟倫]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content judge            pred\n",
              "25165  法官陳嘉惠\\r\\n右正本證明與原本無異\\r\\n中華民國88年5月31日\\r\\n法院\\n上抄本...    []           [陳嘉惠]\n",
              "25461  情形，應於提起上訴或委任時釋明之。上訴人未依第1項、第2項規定委任訴訟代理人，或雖依第2項委...    []           [羅立德]\n",
              "25462  6,002元。茲命上訴人於收受本裁定正本送達後5日內補正委任律師或具律師資格關係人之委任狀及...    []           [羅立德]\n",
              "25463  萬5,655元。茲命上訴人於收受本裁定正本之日起5日內，如數向本院補繳，並提出委任律師或具律...    []           [何君豪]\n",
              "25466  109年12月24日上午9時30分在本院第12法庭行準備程序，特此裁定。中華民國109年11...    []           [許勻睿]\n",
              "25473  律師為訴訟代理人。但上訴人或其法定代理人具有律師資格者，不在此限。上訴人之配偶、三親等內之血...    []  [朱美[UNK], 何君豪]\n",
              "25719  庭法官黃書苑\\r\\n右正本證明與原本無異\\r\\n本裁定不得抗告\\r\\n中華民國八十九年一月五...    []           [黃書苑]\n",
              "25876  B法官黃麟倫\\r\\n右正本證明與原本無異。\\r\\n對於本件判決如有不服，應於收受送達後二十日...    []           [黃麟倫]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "vu0OtyorVZ1z",
        "outputId": "5b4a8c97-d37a-4769-f1fd-5806ec8826b2"
      },
      "source": [
        "patt_notequal_model.iloc[:, 2:5].head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>judge</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25301</th>\n",
              "      <td>附帶民事訴訟裁定\\r\\n109年度簡上附民字第15號\\r\\n附民原告鄧玉恆\\r\\n附民被告劉...</td>\n",
              "      <td>[粘凱庭, 沈婷勻]</td>\n",
              "      <td>[粘凱, 沈婷勻]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25476</th>\n",
              "      <td>花\\r\\n訴訟代理人楊金順律師\\r\\n複代理人吳奎新律師\\r\\n被上訴人\\r\\n即上訴人正昇...</td>\n",
              "      <td>[蔡烱燉, 高孟焄]</td>\n",
              "      <td>[蔡[UNK]燉, 高孟[UNK]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25492</th>\n",
              "      <td>前段分別定有明文。則依此規定，被上訴人羅玉惠、黃天保、許見\\r\\n清、彭武彬分別請求上訴人給...</td>\n",
              "      <td>[蔡烱燉, 王聖惠]</td>\n",
              "      <td>[蔡[UNK]燉, 王聖惠]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25493</th>\n",
              "      <td>事實，因之系爭土地既業經重慶國小種植樹木\\r\\n使用中，本件房屋已無在原址重建回復原狀之可能...</td>\n",
              "      <td>[王聖惠, 蔡烱燉]</td>\n",
              "      <td>[王聖惠, 蔡[UNK]燉]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25713</th>\n",
              "      <td>北地方法院交通法庭\\r\\n法官鍾華\\r\\n右正本證明與原本無異。\\r\\n如不服本裁定，應於裁...</td>\n",
              "      <td>[鍾華]</td>\n",
              "      <td>[鍾華右]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content  ...                pred\n",
              "25301  附帶民事訴訟裁定\\r\\n109年度簡上附民字第15號\\r\\n附民原告鄧玉恆\\r\\n附民被告劉...  ...           [粘凱, 沈婷勻]\n",
              "25476  花\\r\\n訴訟代理人楊金順律師\\r\\n複代理人吳奎新律師\\r\\n被上訴人\\r\\n即上訴人正昇...  ...  [蔡[UNK]燉, 高孟[UNK]]\n",
              "25492  前段分別定有明文。則依此規定，被上訴人羅玉惠、黃天保、許見\\r\\n清、彭武彬分別請求上訴人給...  ...      [蔡[UNK]燉, 王聖惠]\n",
              "25493  事實，因之系爭土地既業經重慶國小種植樹木\\r\\n使用中，本件房屋已無在原址重建回復原狀之可能...  ...      [王聖惠, 蔡[UNK]燉]\n",
              "25713  北地方法院交通法庭\\r\\n法官鍾華\\r\\n右正本證明與原本無異。\\r\\n如不服本裁定，應於裁...  ...               [鍾華右]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    }
  ]
}